# Announcements and Updates

## 2<sup>nd</sup> Edition - Evaluation Report and Ranking (3.21.2025)
The competition report and the ranking is out. Here is the overal ranking, while more details can be found in the competition reports ([ICST preprint](reports/UAV_Competition_ICST_2025.pdf), [SBFT preprint](reports/UAV_Competition_SBFT_2025.pdf)) and the [evaluation artifacts](https://doi.org/10.5281/zenodo.14714469).

|      **Tool**     | **Venue** | **#Obst.** |  **CS2** |              |              | **CS4** |              |              | **CS5** |              |              |    **SUM**    |              | **Final Rank** |
|:-----------------:|:---------:|:----------:|:--------:|:------------:|:------------:|:-------:|:------------:|:------------:|:-------:|:------------:|:------------:|:-------------:|:------------:|:--------------:|
|                   |           |            |  #tests  |    failure   |   diversity  |  #tests |    failure   |   diversity  |  #tests |    failure   |   diversity  |    failure    |   diversity  |                |
| **Pseudo-Random** |    SBFT   |      2     |   31 (7) |   5.25 (2)   |   0.86 (4)   | 66 (12) | **9.30 (1)** |   0.87 (3)   |  17 (5) |   2.71 (2)   |   0.82 (4)   | **17.26 (5)** |   2.57 (11)  |  **6.50 (1)**  |
|    **Evolv-1**    |    ICST   |      2     |  57 (12) | **9.02 (1)** |   0.88 (3)   |  22 (2) |   1.08 (5)   |   0.82 (4)   |  37 (6) | **2.93 (1)** |   0.85 (3)   |   13.04 (7)   |   2.57 (10)  |    7.75 (2)    |
|    **TGen-UQ**    |    ICST   |      2     |   5 (1)  |   0.50 (4)   | **1.00 (1)** |  6 (1)  |   1.44 (4)   | **0.94 (1)** |  6 (0)  |   0.00 (5)   |   0.89 (2)   |    1.95 (13)  | **2.84 (4)** |    10.75 (3)   |
|      **PALM**     |    ICST   |     2,3    |  58 (1)  |   0.14 (6)   |   0.57 (5)   | 82 (20) |   3.58 (2)   |   0.42 (5)   | 76 (19) |   0.80 (3)   |   0.46 (5)   |   4.52 (11)   |   1.47 (15)  |    12.00 (4)   |
|   **Surrealist**  |  Baseline |      2     |  52 (11) |   4.16 (3)   |   0.21 (6)   |  9 (3)  |   1.93 (3)   |   0.20 (6)   |   9 (1) |   0.11 (4)   |   0.18 (6)   |   6.21 (10)   |   0.60 (18)  |    12.00 (4)   |
|  **OptObstacles** |    SBFT   |     2,3    |   3 (1)  |   0.44 (5)   |   0.93 (2)   |  7 (0)  |   0.00 (6)   |   0.92 (2)   |  2 (0)  |   0.00 (5)   | **0.89 (1)** |    0.44 (16)  |   2.75 (5)   |    13.25 (6)   |

## 1<sup>st</sup> Edition - Evaluation Report and Ranking (3.03.2024)

The competition report and the ranking is out. Here is the overal ranking, while more details can be found in the competition report ([preprint](reports/UAV_Competition_SBFT_2024.pdf)).

| Tool Name                                                                             | #obst.     | CS2 |       | CS3 |       | CS4 |       | CS5 |       | CS6 |       | CS7 |       | Rank Sum  | Score Sum  | Final Rank  |
|---------------------------------------------------------------------------------------|------------|-----|-------|-----|-------|-----|-------|-----|-------|-----|-------|-----|-------|-----------|------------|-------------|
|                                                                                       |            | #   | score | #   | score | #   | score | #   | score | #   | score | #   | score |           |            |             |
| [WOGAN-UAV](https://gitlab.abo.fi/stc/experiments/wogan-uav)                          | 3          | 61  | 12,55 | 72  | 14,00 | 81  | 2,35  | 39  | 8,40  | 71  | 4,81  | 90  | 11,57 | 12        | 53,69      | 1           |
| [TUMB](https://github.com/MayDGT/UAV-Testing-Competition)                             | [1-4]      | 69  | 0,12  | 113 | 15,59 | 135 | 7,12  | 114 | 2,73  | 151 | 15,32 | 125 | 11,12 | 16        | 52,00      | 2           |
| [CAMBA](https://github.com/mdeliso97/CAMBA_CPS-UAV_at_the_SBFT_Tool_Competition_2024) | 2          | 36  | 11,84 | 30  | 8,50  | 33  | 0,00  | 11  | 3,16  | 102 | 12,92 | 22  | 4,69  | 18        | 41,11      | 3           |
| [DeepHyperion-UAV](https://github.com/zohdit/UAV-Testing-Competition)                 | 2          | 2   | 1,22  | 28  | 8,31  | 10  | 7,74  | 22  | 1,08  | 7   | 0,00  | 14  | 2,96  | 28        | 21,31      | 4           |
| [AmbiGen](https://github.com/swat-lab-optimization/UAV-Testing-Competition)           | [1-4]      | 30  | 2,82  | 46  | 2,00  | 36  | 0,86  | 65  | 1,22  | 151 | 10,07 | 30  | 1,51  | 26,00     | 18,47      | 5           |
| [Surrealist](https://github.com/skhatiri/Surrealist)                                  | 2          | 10  | 5,03  | 1   | 0,00  | 10  | 0,00  | 1   | 0,75  | 19  | 3,88  | 2   | 0,00  | 34        | 9,67       | 6           |
| [TAIiST](https://github.com/Trusted-AI-in-System-Test/UAV-Testing-Competition)        | [2-4]      | 7   | 0,81  | 13  | 0,00  | 6   | 0,43  | 11  | 1,57  | 29  | 1,67  | 22  | 1,33  | 33        | 5,81       | 7           |
| **SUM**                                                                               |            | 215 | 34,40 | 303 | 48,39 | 311 | 18,51 | 263 | 18,91 | 530 | 48,67 | 305 | 33,19 |           |            |             |

## Important Updates (8.12.2023)

- **Due to multiple requests, we are implementing a grace period. You can update your submissions until 10.12.2023 (AoE).**
- **In the meantime, we are checking if we can run and evaluate your submitted codes. We will let you know if we got problems and ask for an update if needed.**
- **A small update to [testcase.py](https://github.com/skhatiri/UAV-Testing-Competition/commit/50e3e983ef98f1be4a550922fc23b68b35740a4b) will also help some of you that had problems integrating latest changes.**
- **To easily set the simulation timeout, you can use the following command to run your test generator:**

  `SIMULATION_TIMEOUT=300 python3 cli.py generate case_studies/mission1.yaml 100`

## Important Updates (4.12.2023)

- **Recent [Aerialist update](https://github.com/skhatiri/Aerialist/pull/14) lets you set a timeout period for simulations. After timeout is reached, simulation is aborted and you can access the flight log and plot of the unfinished test.**
- **This can help with identifying the test cases that take too much time to finish, and before you were not able to check the logs to understand why.**

## Important Updates (28.11.2023)

- **Sumbmission Deadline is extended to 7.12.2023 (AoE).**
- **Make Sure to follow the below requirements in your submission.**

## Important Updates (21.11.2023)

- **Submission requirements and guideline is now available. [Check Here](./docs/submission.md).**
- **Make sure to review and integrate the recent updates to the [code samples](./snippets/) into your code where needed.**
- **Make sure to pull/install the latest version of [Aerialist](https://github.com/skhatiri/Aerialist).**
